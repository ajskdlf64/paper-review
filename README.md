# paper-review

1. [Training language models to follow instructions with human feedback](https://github.com/ajskdlf64/paper-review/blob/main/Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.md)
2. [Zero-Resource Cross-Domain Named Entity Recognition](https://github.com/ajskdlf64/paper-review/blob/main/Zero-Resource%20Cross-Domain%20Named%20Entity%20Recognition.md)
3. [Zero-Resource Cross-Lingual Named Entity Recognition](https://github.com/ajskdlf64/paper-review/blob/main/Zero-Resource%20Cross-Lingual%20Named%20Entity%20Recognition.md)
4. ViT : [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
](https://github.com/ajskdlf64/paper-review/blob/main/An%20Image%20is%20Worth%2016x16%20Words:%20Transformers%20for%20Image%20Recognition%20at%20Scale.md)
5. CAM : [Learning Deep Features for Discriminative Localization](https://github.com/ajskdlf64/paper-review/blob/main/Learning%20Deep%20Features%20for%20Discriminative%20Localization.md)
6. MoE : [OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://github.com/ajskdlf64/paper-review/blob/main/OUTRAGEOUSLY%20LARGE%20NEURAL%20NETWORKS:%20THE%20SPARSELY-GATED%20MIXTURE-OF-EXPERTS%20LAYER.md)
7. Swin Transformer : [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
](https://github.com/ajskdlf64/paper-review/blob/main/Swin%20Transformer:%20Hierarchical%20Vision%20Transformer%20using%20Shifted%20Windows.md)
